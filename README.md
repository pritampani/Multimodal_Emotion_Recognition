# Multimodal_Emotion_Recognition


This project focuses on building a Multimodal Emotion Detection system using **textual data** (as the first modality), with future extensions planned for **audio** and **visual** modalities. The goal is to accurately identify human emotions in conversations by leveraging natural language processing (NLP) and deep learning techniques.

---

## ğŸ§  Project Objective

To build a deep learning-based **Multimodal Emotion Recognition (MER)** model, starting with training on **textual features** (e.g., BERT embeddings) and expanding to **multimodal fusion** using data from audio and video sources.

---

## ğŸ“¦ Features

- ğŸ” Emotion classification from text using pre-trained transformers (BERT, RoBERTa, etc.)
- ğŸ§ª Fusion-ready architecture to incorporate audio/visual data
- ğŸ—‚ï¸ Modular codebase: easy to plug in new modalities
- ğŸ“Š Evaluation with standard benchmarks: IEMOCAP, MELD, MOSEI

---

## ğŸ—ï¸ Project Architecture

```plaintext
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Raw Text    â”‚
           â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Tokenizer  â”‚
          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ BERT Embeddings  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Classifier   â”‚  â† (LSTM / FFN / Softmax)
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Emotion Tag  â”‚ â†’ [Happy, Sad, Angry, Neutral, etc.]
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



## ğŸ“š Datasets Used
	â€¢	IEMOCAP
	â€¢	MELD
	â€¢	CMU-MOSEI


ğŸ”§ Setup Instructions
# Clone the repo
git clone https://github.com/your-username/multimodal-emotion-detection.git
cd multimodal-emotion-detection

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or .\venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt


## ğŸš€ Training (Text Modality)
python train_text_model.py --model bert --dataset iemocap
Options:
	â€¢	--model: [bert, roberta, distilbert]
	â€¢	--dataset: [iemocap, meld, mosei]

## ğŸ“ˆ Evaluation

python evaluate.py --checkpoint saved_models/text_bert.pth

Metrics:
	â€¢	Accuracy
	â€¢	Precision, Recall, F1
	â€¢	Confusion Matrix

## ğŸ› ï¸ Future Work
	â€¢	Integrate Audio modality (OpenSMILE + CNN/RNN)
	â€¢	Integrate Video modality (3D CNNs or facial expression detectors)
	â€¢	Fusion via Attention, MLP, or Gated Networks
	â€¢	Real-time prediction API

## ğŸ“„ References
	â€¢	MEmoBERT: Prompt-based Multimodal Emotion Recognition
	â€¢	Survey: Multimodal Emotion Recognition in Conversations (2025)
	â€¢	CMU-MOSEI Dataset

â¸»

## ğŸ¤ Contributing

Feel free to open issues or submit pull requests to improve the project.

## ğŸ“ƒ License

MIT License Â© 2025 Pritam Pani
